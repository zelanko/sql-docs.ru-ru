---
title: SUSE. Настройка группы доступности для SQL Server на Linux
titleSuffix: SQL Server
description: Узнайте, как создавать кластеры групп доступности SQL Server в SUSE Linux Enterprise Server (SLES).
author: MikeRayMSFT
ms.author: mikeray
ms.reviewer: vanto
ms.date: 04/30/2018
ms.topic: conceptual
ms.prod: sql
ms.technology: linux
ms.assetid: 85180155-6726-4f42-ba57-200bf1e15f4d
ms.openlocfilehash: c6c5ecf91349a94acb2b18156f28056ce04da3a1
ms.sourcegitcommit: f7ac1976d4bfa224332edd9ef2f4377a4d55a2c9
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/02/2020
ms.locfileid: "85892332"
---
# <a name="configure-sles-cluster-for-sql-server-availability-group"></a>Настройка кластера SLES для группы доступности SQL Server

[!INCLUDE [SQL Server - Linux](../includes/applies-to-version/sql-linux.md)]

Это руководство содержит инструкции по созданию кластера с тремя узлами для SQL Server на основе SUSE Linux Enterprise Server (SLES) 12 с пакетом обновления 2 (SP2). Для обеспечения высокого уровня доступности группе доступности в Linux требуется три узла — см. статью [Высокий уровень доступности и защита данных для конфигураций групп доступности](sql-server-linux-availability-group-ha.md). Уровень кластеризации основан на [расширении высокого уровня доступности (HAE)](https://www.suse.com/products/highavailability), построенном на основе [Pacemaker](https://clusterlabs.org/). 

Дополнительные сведения о конфигурации кластера, параметрах агента ресурсов, управлении и рекомендациях см. в статье [Расширение высокого уровня доступности для SUSE Linux Enterprise 12 с пакетом обновления 2 (SP2)](https://www.suse.com/documentation/sle-ha-12/index.html).

>[!NOTE]
>На этом этапе интеграция SQL Server с Pacemaker в Linux реализована не на таком уровне, как с WSFC в Windows. Служба SQL Server в Linux не поддерживает кластеры. Pacemaker управляет всеми задачами оркестрации ресурсов кластера, включая ресурс группы доступности. В Linux не следует полагаться на динамические административные представления группы доступности Always On, которые предоставляют сведения о кластере, такие как sys.dm_hadr_cluster. Кроме того, имя виртуальной сети относится только к WSFC. В Pacemaker его эквивалент отсутствует. Вы по-прежнему можете создать прослушиватель, чтобы использовать его для прозрачного переподключения после отработки отказа, но требуется вручную зарегистрировать имя прослушивателя на DNS-сервере с IP-адресом, использованным для создания ресурса виртуального IP-адреса (как описано в следующих разделах).

[!INCLUDE [bias-sensitive-term-t](../includes/bias-sensitive-term-t.md)]

## <a name="roadmap"></a>Схема действий

Процедура создания группы доступности для обеспечения высокого уровня доступности на серверах Linux отличается от процедуры для отказоустойчивого кластера Windows Server. Ниже описывается общий порядок действий. 

1. [Настройте SQL Server в узлах кластера](sql-server-linux-setup.md).

2. [Создайте группу доступности](sql-server-linux-availability-group-failover-ha.md). 

3. Настройте диспетчер ресурсов кластера, например Pacemaker. Эти инструкции приведены в этом документе.
   
   Способ настройки диспетчера ресурсов кластера зависит от конкретного дистрибутива Linux. 

   >[!IMPORTANT]
   >Для обеспечения высокого уровня доступности в рабочих средах требуется агент ограждения, например STONITH. В примерах в этой статье агенты ограждения не применяются. Они служат только для тестирования и проверки. 
   
   >Кластер Pacemaker использует ограждение для возврата кластера в известное состояние. Способ настройки ограждения зависит от дистрибутива и среды. В настоящее время ограждение недоступно в некоторых облачных средах. См. статью о [расширении для обеспечения высокого уровня доступности SUSE Linux Enterprise](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.fencing).

5. [Добавьте группу доступности в виде ресурса в кластере](sql-server-linux-availability-group-cluster-sles.md#configure-the-cluster-resources-for-sql-server). 

## <a name="prerequisites"></a>Предварительные требования

Для выполнения описанного ниже сценария нужны три компьютера для развертывания кластера с тремя узлами. Ниже описаны общие действия по настройке этих серверов.

## <a name="setup-and-configure-the-operating-system-on-each-cluster-node"></a>Установка и настройка операционной системы в каждом узле кластера 

Сначала необходимо настроить операционную систему в узлах кластера. Для этого пошагового руководства используйте SLES 12 с пакетом обновления 2 (SP2) с действительной подпиской на надстройку высокого уровня доступности.

### <a name="install-and-configure-sql-server-service-on-each-cluster-node"></a>Установка и настройка службы SQL Server в каждом узле кластера

1. Установите и настройте службу SQL Server во всех узлах. Подробные инструкции см. в статье [Установка SQL Server в Linux](sql-server-linux-setup.md).

1. Назначьте один узел первичным, а остальные — вторичными. Эти термины будут применяться далее в данном руководстве.

1. Убедитесь в том, что узлы, которые будут входить в кластер, могут взаимодействовать друг с другом.

   В следующем примере показан файл `/etc/hosts` с дополнениями для трех узлов: SLES1, SLES2 и SLES3.

   ```
   127.0.0.1   localhost
   10.128.16.33 SLES1
   10.128.16.77 SLES2
   10.128.16.22 SLES3
   ```

   У всех узлов кластера должен быть доступ друг к другу через SSH. Таким средствам, как `hb_report` или `crm_report` (для устранения неполадок) и обозреватель журнала Hawk, не нужен пароль для доступа по SSH между узлами. В противном случае они могут только собирать данные из текущего узла. Если вы используете нестандартный порт SSH, воспользуйтесь параметром -X (см. страницу `man`). Например, если вы используете порт SSH 3479, вызовите средство `crm_report` с помощью следующей команды:

   ```bash
   sudo crm_report -X "-p 3479" [...]
   ```

   Дополнительные сведения см. в [разделе "Прочее" руководства по администрированию SLES](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.troubleshooting.misc).


## <a name="create-a-sql-server-login-for-pacemaker"></a>Создание учетных данных SQL Server для Pacemaker

[!INCLUDE [SLES-Create-SQL-Login](../includes/ss-linux-cluster-pacemaker-create-login.md)]

## <a name="configure-an-always-on-availability-group"></a>Настройка группы доступности Always On

На серверах Linux настройте группу доступности, а затем настройте кластерные ресурсы. Инструкции по настройке группы доступности см. в статье [Настройка группы доступности Always On для SQL Server на Linux](sql-server-linux-availability-group-configure-ha.md).

## <a name="install-and-configure-pacemaker-on-each-cluster-node"></a>Установка и настройка Pacemaker в каждом узле кластера

1. Установите расширение высокого уровня доступности.

   Справочные сведения см. в статье [Установка SUSE Linux Enterprise Server и расширения для обеспечения высокого уровня доступности](https://www.suse.com/documentation/sle-ha-12/singlehtml/install-quick/install-quick.html#sec.ha.inst.quick.installation).

1. Установите пакет агента ресурсов SQL Server в обоих узлах.

   ```bash
   sudo zypper install mssql-server-ha
   ```

## <a name="set-up-the-first-node"></a>Настройка первого узла

   См. [инструкции по установке SLES](https://www.suse.com/documentation/sle-ha-12/singlehtml/install-quick/install-quick.html#sec.ha.inst.quick.setup.1st-node).

1. Войдите на физический компьютер или в виртуальную машину, которые необходимо использовать в качестве узла кластера, как пользователь `root`.
2. Запустите скрипт начальной загрузки с помощью следующей команды:
   ```bash
   sudo ha-cluster-init
   ```

   Если не настроен запуск NTP во время загрузки, появится сообщение. 

   Если вы решите продолжить, несмотря на это, скрипт автоматически создаст ключи для доступа по протоколу SSH и для средства синхронизации Csync2, а затем запустит необходимые для них службы. 

3. Чтобы настроить слой связи кластера (Corosync), выполните указанные ниже действия. 

   а. Введите сетевой адрес для привязки. По умолчанию скрипт предлагает сетевой адрес eth0. Можно ввести другой сетевой адрес, например bond0. 

   b. Введите адрес многоадресной рассылки. Скрипт предлагает случайный адрес, который можно использовать по умолчанию. 

   c. Введите порт многоадресной рассылки. Сценарий предлагает порт 5405 по умолчанию. 

   d. Чтобы настроить `SBD ()`, введите постоянный путь к разделу блочного устройства, которое следует использовать для SBD. Путь должен быть одинаковым во всех узлах кластера. 
   Наконец, скрипт запустит службу Pacemaker, чтобы перевести кластер с одним узлом в оперативный режим и включить веб-интерфейс управления Hawk2. URL-адрес, используемый для Hawk2, отображается на экране. 

4. Подробные сведения о процессе установки см. в файле `/var/log/sleha-bootstrap.log`. Теперь у вас есть работающий кластер с одним узлом. Проверьте состояние кластера с помощью команды crm status:

   ```bash
   sudo crm status
   ```

   Можно также просмотреть конфигурацию кластера с помощью команды `crm configure show xml` или `crm configure show`.

5. Процедура начальной загрузки создает пользователя Linux с именем hacluster и паролем linux. Замените пароль по умолчанию надежным как можно скорее. 

   ```bash
   sudo passwd hacluster
   ```

## <a name="add-nodes-to-the-existing-cluster"></a>Добавление узлов в существующий кластер

В работающий кластер с одним или несколькими узлами можно добавить дополнительные узлы с помощью скрипта начальной загрузки ha-cluster-join. Скрипту требуется доступ только к существующему узлу кластера. Он автоматически выполняет базовую установку на текущем компьютере. Выполните указанные ниже действия.

Если вы настроили существующие узлы кластера с помощью модуля кластера `YaST`, перед запуском скрипта `ha-cluster-join` убедитесь в том, что выполнены указанные ниже необходимые условия.
- Привилегированный пользователь в существующих узлах имеет ключи SSH для входа без пароля. 
- В существующих узлах настроено средство `Csync2`. Дополнительные сведения см. в разделе о настройке Csync2 с YaST. 

1. Войдите на физический компьютер или в виртуальную машину, которые необходимо присоединить к кластеру, как привилегированный пользователь. 
2. Запустите скрипт начальной загрузки с помощью следующей команды: 

   ```bash
   sudo ha-cluster-join
   ```

   Если не настроен запуск NTP во время загрузки, появится сообщение. 

3. Если вы решите продолжить, несмотря на это, необходимо будет ввести IP-адрес существующего узла. Введите IP-адрес. 

4. Если вы еще не настроили доступ по протоколу SSH без пароля между двумя компьютерами, вам также будет предложено ввести пароль привилегированного пользователя существующего узла. 

   После входа в указанный узел скрипт копирует конфигурацию Corosync, настраивает SSH и `Csync2` и переводит текущий компьютер в режим "в сети" в качестве нового узла кластера. Помимо этого, он запускает службу, необходимую для Hawk. Если вы настроили общее хранилище с `OCFS2`, также автоматически создается каталог точки подключения для файловой системы `OCFS2`. 

5. Повторите предыдущие действия для всех компьютеров, которые требуется добавить в кластер. 

6. Подробные сведения о процессе см. в файле `/var/log/ha-cluster-bootstrap.log`. 

1. Проверьте состояние кластера с помощью команды `sudo crm status`. Если второй узел добавлен успешно, выходные данные должны быть примерно следующими:

   ```bash
   sudo crm status
   
   3 nodes configured
   1 resource configured
   Online: [ SLES1 SLES2 SLES3]
   Full list of resources:   
   admin_addr     (ocf::heartbeat:IPaddr2):       Started node1
   ```

   >[!NOTE]
   >`admin_addr` — это виртуальный кластерный ресурс IP-адреса, который настраивается в процессе первоначальной установки кластера с одним узлом.

После добавления всех узлов проверьте, нужно ли настроить политику без кворума в глобальных параметрах кластера. Это особенно важно для кластеров с двумя узлами. Дополнительные сведения см. в разделе 4.1.2 "Параметр no-quorum-policy". 

## <a name="set-cluster-property-cluster-recheck-interval"></a>Задание свойства кластера cluster-recheck-interval

`cluster-recheck-interval` указывает интервал опроса, с которым кластер проверяет наличие изменений в параметрах ресурсов, ограничениях или других параметрах кластера. Если реплика выходит из строя, кластер пытается перезапустить ее с интервалом, который связан со значениями `failure-timeout` и `cluster-recheck-interval`. Например, если для `failure-timeout` установлено значение 60 с, а для `cluster-recheck-interval` — 120 с, то повторная попытка перезапуска предпринимается с интервалом, который больше 60 с, но меньше 120 с. Мы рекомендуем установить для failure-timeout значение, равное 60 с, а для cluster-recheck-interval значение больше 60 с. Задавать для cluster-recheck-interval небольшое значение не рекомендуется.

Чтобы изменить значение свойства на `2 minutes`, выполните следующую команду:

```bash
crm configure property cluster-recheck-interval=2min
```

> [!IMPORTANT] 
> Если у вас уже есть ресурс группы доступности, управляемый кластером Pacemaker, обратите внимание, что все дистрибутивы, использующие последний доступный пакет Pacemaker 1.1.18-11.el7, вносят изменение в поведение для параметра start-failure-is-fatal cluster, когда его значение равно false. Это изменение влияет на рабочий процесс отработки отказа. В случае сбоя первичной реплики ожидается, что будет выполнена отработка отказа кластера на одну из доступных вторичных реплик. Вместо этого пользователи увидят, что кластер продолжает попытки запустить первичную реплику с ошибкой. Если первичная реплика не включается (из-за постоянного сбоя), кластер не выполняет отработку отказа на другую доступную вторичную реплику. Из-за этого изменения рекомендуемая ранее конфигурация для установки параметра start-failure-is-fatal больше не действительна, и для этого параметра нужно вернуть значение по умолчанию `true`. Кроме того, нужно обновить ресурс группы доступности для включения свойства `failover-timeout`. 
>
>Чтобы изменить значение свойства на `true`, выполните следующую команду:
>
>```bash
>crm configure property start-failure-is-fatal=true
>```
>
>Измените существующее свойство `failure-timeout` ресурса группы доступности на выполнение `60s` (замените `ag1` именем ресурса группы доступности). 
>
>```bash
>crm configure edit ag1
># In the text editor, add `meta failure-timeout=60s` after any `param`s and before any `op`s
>```

Дополнительные сведения о свойствах кластера Pacemaker см. в статье [Настройка ресурсов кластера](https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_config_crm_resources.html).

## <a name="configure-fencing-stonith"></a>Настройка ограждения (STONITH)
Поставщики кластера Pacemaker требуют, чтобы STONITH был включен, а устройство ограждения настроено для поддерживаемой установки кластера. Если диспетчер ресурсов кластера не может определить состояние узла или ресурса в нем, для перевода кластера в известное состояние используется ограждение.

Ограждение на уровне ресурсов гарантирует отсутствие повреждений данных во время сбоя за счет настройки ресурса. Вы можете использовать ограждение на уровне ресурсов, например с распределенным реплицируемым блочным устройством (DRBD), чтобы пометить диск в узле как устаревший при отключении канала связи.

Ограждение на уровне узлов гарантирует, что в узле не выполняются никакие ресурсы. Это осуществляется путем сброса узла, а соответствующая реализация для Pacemaker называется STONITH (что дословно расшифровывается как "застрелить другой узел"). Pacemaker поддерживает множество разных устройств ограждения, таких как источник бесперебойного питания или карты интерфейса управления для серверов.

Дополнительные сведения см. в разделе:

- [Кластеры Pacemaker с нуля](https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Clusters_from_Scratch/)
- [Ограждение и STONITH](https://clusterlabs.org/doc/crm_fencing.html)
- [Документация по высокому уровню доступности SUSE: ограждение и STONITH](https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_fencing.html)

Во время инициализации кластера STONITH отключается, если конфигурация не обнаружена. Его можно включить позже, выполнив следующую команду:

```bash
sudo crm configure property stonith-enabled=true
```
  
>[!IMPORTANT]
>Отключение STONITH выполняется только в целях тестирования. Если вы планируете использовать Pacemaker в рабочей среде, следует спланировать реализацию STONITH с учетом особенностей среды и поддерживать ее в рабочем состоянии. SUSE не предоставляет агенты ограждения для каких-либо облачных сред (включая Azure) или Hyper-V. Следовательно, поставщик кластера не предоставляет поддержку для запуска рабочих кластеров в этих средах. Мы работаем над этой проблемой. Решение будет доступно в будущих выпусках.

## <a name="configure-the-cluster-resources-for-sql-server"></a>Настройка ресурсов кластера для SQL Server

См. [руководство по администрированию SLES](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.manual_config).

## <a name="enable-pacemaker"></a>Включение Pacemaker

Включите автоматический запуск Pacemaker.

Выполните приведенную ниже команду в каждом узле кластера.

```bash
systemctl enable pacemaker
```

### <a name="create-availability-group-resource"></a>Создание ресурса группы доступности

Приведенная ниже команда создает и настраивает ресурс группы доступности для трех реплик группы доступности [ag1]. Операции мониторинга и значения времени ожидания необходимо задать в SLES явно, так как время ожидания сильно зависит от рабочей нагрузки и его необходимо тщательно подбирать для каждого развертывания.
Выполните следующую команду в одном из узлов кластера:

1. Выполните команду `crm configure`, чтобы открыть командную строку crm:

   ```bash
   sudo crm configure 
   ```

1. В командной строке crm выполните приведенную ниже команду, чтобы настроить свойства ресурса.

   ```bash
   primitive ag_cluster \
      ocf:mssql:ag \
      params ag_name="ag1" \
      meta failure-timeout=60s \
      op start timeout=60s \
      op stop timeout=60s \
      op promote timeout=60s \
      op demote timeout=10s \
      op monitor timeout=60s interval=10s \
      op monitor timeout=60s interval=11s role="Master" \
      op monitor timeout=60s interval=12s role="Slave" \
      op notify timeout=60s
   ms ms-ag_cluster ag_cluster \
      meta master-max="1" master-node-max="1" clone-max="3" \
     clone-node-max="1" notify="true" \
   commit
      ```

[!INCLUDE [required-synchronized-secondaries-default](../includes/ss-linux-cluster-required-synchronized-secondaries-default.md)]

### <a name="create-virtual-ip-resource"></a>Создание ресурса виртуального IP-адреса

Если вы не создали ресурс виртуального IP-адреса при выполнении `ha-cluster-init`, его можно создать сейчас. Указанная ниже команда создает ресурс виртуального IP-адреса. Замените `<**0.0.0.0**>` доступным в сети адресом, а `<**24**>` — числом битов в маске подсети CIDR. Выполните команду в одном узле.

```bash
crm configure \
primitive admin_addr \
   ocf:heartbeat:IPaddr2 \
   params ip=<**0.0.0.0**> \
      cidr_netmask=<**24**>
```

### <a name="add-colocation-constraint"></a>Добавление ограничения совместного размещения
Почти каждое решение в кластере Pacemaker, например выбор места запуска ресурса, принимается путем сравнения оценок. Оценки вычисляются для каждого ресурса, а диспетчер ресурсов кластера выбирает узел с наивысшей оценкой для конкретного ресурса. (Если узел имеет отрицательную оценку для ресурса, последний не может выполняться в этом узле.) Решениями для кластера можно управлять с помощью ограничений. Ограничения имеют оценку. Если ограничение имеет оценку меньше бесконечности (INFINITY), то это только рекомендация. Оценка, равная INFINITY, указывает на обязательный характер. Чтобы первичная реплика группы доступности и ресурс виртуального IP-адреса находились в одном узле, определите ограничение совместного размещения с оценкой INFINITY. 

Чтобы задать ограничение совместного размещения виртуального IP-адреса в одном узле с главным экземпляром, выполните следующую команду в одном узле:

```bash
crm configure
colocation vip_on_master inf: \
    admin_addr ms-ag_cluster:Master
commit
```

### <a name="add-ordering-constraint"></a>Добавление ограничения упорядочения
Ограничение совместного размещения имеет неявное ограничение упорядочения. Оно перемещает ресурс виртуального IP-адреса перед перемещением ресурса группы доступности. Последовательность событий по умолчанию: 

1. Пользователь выполняет команду переноса ресурса из узла node1 в узел node2 для главной реплики группы доступности.
2. Ресурс виртуального IP-адреса останавливается в узле 1.
3. Ресурс виртуального IP-адреса запускается в узле 2. На этом этапе IP-адрес временно указывает на узел 2, пока узел 2 все еще является вторичной репликой перед отработкой отказа. 
4. Уровень главной реплики группы доступности в узле 1 понижается.
5. Реплика группы доступности в узле 2 повышается до главной. 

Чтобы предотвратить ситуацию, когда IP-адрес временно указывает на узел с вторичной репликой перед отработкой отказа, добавьте ограничение упорядочения. Чтобы добавить ограничение упорядочения, выполните приведенную ниже команду в одном узле. 

```bash
crm crm configure \
   order ag_first inf: ms-ag_cluster:promote admin_addr:start
```


>[!IMPORTANT]
>После настройки кластера и добавления группы доступности в качестве ресурса кластера вы не можете использовать Transact-SQL для отработки отказа ресурсов группы доступности. Ресурсы кластера SQL Server в Linux не так сильно зависят от операционной системы, как если бы они находились в отказоустойчивом кластере Windows Server (WSFC). Служба SQL Server не имеет сведений о наличии кластера. Вся оркестрация осуществляется с помощью средств управления кластерами. В SLES используйте `crm`. 

Вручную выполните отработку отказа группы доступности с помощью `crm`. Не инициируйте отработку отказа с помощью Transact-SQL. Дополнительные сведения см. в разделе [Отработка отказа](sql-server-linux-availability-group-failover-ha.md#failover).


Дополнительные сведения см. в разделе:
- [Управление ресурсами кластера](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.config.crm)   
- [Основные понятия высокого уровня доступности](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.concepts)
- [Краткий справочник по Pacemaker](https://github.com/ClusterLabs/pacemaker/blob/master/doc/pcs-crmsh-quick-ref.md) 

<!---[!INCLUDE [Pacemaker Concepts](..\includes\ss-linux-cluster-pacemaker-concepts.md)]--->

## <a name="next-steps"></a>Дальнейшие действия

[Использование высокодоступной группы доступности](sql-server-linux-availability-group-failover-ha.md)
