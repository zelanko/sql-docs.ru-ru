---
title: Технический справочник по алгоритму нейронной сети Майкрософт | Документы Microsoft
ms.custom: ''
ms.date: 03/14/2017
ms.prod: analysis-services
ms.prod_service: analysis-services
ms.service: ''
ms.component: data-mining
ms.reviewer: ''
ms.suite: pro-bi
ms.technology: ''
ms.tgt_pltfrm: ''
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
caps.latest.revision: 26
author: Minewiskan
ms.author: owend
manager: kfile
ms.openlocfilehash: fed9fc9bfa04c6d4099016b4b0976b1bc72e9d40
ms.sourcegitcommit: 2ddc0bfb3ce2f2b160e3638f1c2c237a898263f4
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 05/03/2018
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a>Технический справочник по алгоритму нейронной сети (Майкрософт)
[!INCLUDE[ssas-appliesto-sqlas](../../includes/ssas-appliesto-sqlas.md)]
  Алгоритм нейронной сети ([!INCLUDE[msCoName](../../includes/msconame-md.md)]) использует сеть в виде *многослойного перцептрона*, также известного под названием *сеть дельта-правила с обратным распространением*, в состав которой может входить до трех слоев нейронов, или *перцептронов*. Такими слоями являются входной слой, необязательный скрытый слой и выходной слой.  
  
 В задачи настоящей документации не входит подробное рассмотрение таких нейронных сетей, как многослойные перцептроны. В данном разделе содержится описание базовой реализации алгоритма, в том числе метода, используемого для нормализации входных и выходных значений. Приводится также описание методов выбора компонентов, которые применяются для снижения количества элементов атрибута. В данном разделе описываются параметры и другие настройки, с помощью которых можно управлять поведением алгоритма. Приводятся также ссылки на дополнительную информацию о запросах к модели.  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a>Реализация алгоритма нейронной сети (Майкрософт)  
 В нейронной сети в виде многослойного перцептрона каждый нейрон получает один или несколько входов, а также создает один или несколько одинаковых выходов. Каждый выход является простой нелинейной функцией суммы входов, полученных нейроном. Входы передаются в прямом направлении от узлов во входном слое к узлам в скрытом слое, а оттуда передаются на выходной слой; нейроны в составе слоя не соединены друг с другом. Если скрытый слой отсутствует, как в модели логистической регрессии, то входы передаются в прямом направлении непосредственно от узлов входного слоя к узлам выходного слоя.  
  
 В нейронной сети, создаваемой с помощью алгоритма нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ), существует три типа нейронов.  
  
**Входные нейроны**  
  
 Входные нейроны предоставляют модели интеллектуального анализа данных значения входных атрибутов. Для дискретных входных атрибутов входной нейрон обычно представляет одно состояние из входного атрибута. Сюда входят также отсутствующие значения, если обучающие данные содержат для этого атрибута значения NULL. Дискретный входной атрибут, имеющий более двух состояний, создает один входной нейрон для каждого состояния и один входной нейрон для отсутствующего состояния, если обучающие данные содержат какие-либо значения NULL. Непрерывный входной атрибут создает два входных нейрона: один нейрон для отсутствующего состояния и один нейрон для значения самого непрерывного атрибута. Входные нейроны обеспечивают входы для одного или нескольких скрытых нейронов.  
  
**Hidden neurons**  
  
 Скрытые нейроны получают входные данные от входных нейронов и передают выходные данные выходным нейронам.  
  
**Выходные нейроны**  
  
 Выходные нейроны представляют значения прогнозируемых атрибутов для модели интеллектуального анализа данных. Для дискретных входных атрибутов выходной нейрон обычно указывает одно прогнозируемое состояние для прогнозируемого атрибута, включая пропущенные значения. Например, бинарный прогнозируемый атрибут создает один выходной узел, который описывает отсутствующее или существующее состояние, указывая на наличие значения для такого атрибута. Логический столбец, используемый как прогнозируемый, создает три выходных нейрона: один нейрон для истинного значения, один для ложного и один для отсутствующего или существующего состояния. Дискретный прогнозируемый атрибут, имеющий более двух состояний, создает один выходной нейрон для каждого состояния и один выходной нейрон для отсутствующего или существующего состояния. Непрерывные прогнозируемые столбцы создают два выходных нейрона: один нейрон для отсутствующего или существующего состояния и один нейрон для значения самого непрерывного столбца. При создании путем просмотра набора прогнозируемых столбцов более 500 выходных нейронов службы [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] формируют для представления дополнительных выходных нейронов новую сеть в модели интеллектуального анализа данных.  
  
 Нейрон получает входы от других нейронов или из других данных, в зависимости от того, в каком слое сети он находится. Входной нейрон получает на входе оригинальные данные. Скрытые нейроны и выходные нейроны получают входы из выхода других нейронов нейронной сети. Входы устанавливают связи между нейронами, и эти связи являются путем, по которому производится анализ для конкретного набора вариантов.  
  
 Каждому входу присвоено значение, именуемое *весом*, которое описывает релевантность или важность конкретного входа для скрытого или выходного нейрона. Чем больше вес, присвоенный входу, тем более релевантным или важным является значение этого входа. Значения веса могут быть отрицательными; это означает, что вход может подавлять, а не активировать конкретный нейрон. Чтобы выделить важность входа для конкретного нейрона, значение входа умножается на вес. В случае отрицательных весов умножение значения на вес служит для уменьшения важности входа.  
  
 Каждому нейрону присвоена простая нелинейная функция, называемая *функцией активации*, которая описывает релевантность или важность определенного нейрона для этого слоя нейронной сети. В качестве функции активации скрытые нейроны используют функцию *гиперболического тангенса* (tanh), а выходные нейроны — *сигмоидальную функцию* . Обе функции являются нелинейными, непрерывными функциями, позволяющими нейронной сети моделировать нелинейные связи между входными и выходными нейронами.  
  
### <a name="training-neural-networks"></a>Обучение нейронных сетей  
 Обучение модели интеллектуального анализа данных, использующей алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ), основано на нескольких шагах. На указанные шаги сильное влияние оказывают заданные значения параметров алгоритма.  
  
 Алгоритм сначала оценивает и извлекает обучающие данные из источника данных. Определенный процент обучающих данных, называемых *контрольными данными*, зарезервирован для использования при определении точности сети. В процессе обучения после каждой итерации по обучающим данным выполняется оценка сети. После прекращения роста точности модели процесс обучения завершается.  
  
 Значения параметров *SAMPLE_SIZE* и *HOLDOUT_PERCENTAGE* используются для определения количества вариантов, которое необходимо выбрать из обучающих данных, а также количества вариантов, которое необходимо зарезервировать для использования в качестве контрольных данных. Значение параметра *HOLDOUT_SEED* используется для определения случайным образом конкретных вариантов, подлежащих резервированию в качестве контрольных данных.  
  
> [!NOTE]  
>  Эти параметры алгоритма отличны от свойств HOLDOUT_SIZE и HOLDOUT_SEED, применяемых к структуре интеллектуального анализа данных для определения набора проверочных данных.  
  
 Затем алгоритм определяет количество и сложность сетей, поддерживаемых моделью интеллектуального анализа данных. Если в модели интеллектуального анализа данных содержится один или несколько атрибутов, используемых только для прогнозирования, то алгоритм создает одну сеть, которая представляет все эти атрибуты. Если в модели интеллектуального анализа данных содержится один или несколько атрибутов, используемых как для ввода, так и для прогнозирования, то поставщик алгоритмов создает сеть для каждого такого атрибута.  
  
 Для входных и прогнозируемых атрибутов, которые имеют дискретные значения, каждый входной или выходной нейрон соответственно представляет одно состояние. Для входных и прогнозируемых атрибутов, которые имеют непрерывные атрибуты, каждый входной или выходной нейрон соответственно представляет диапазон и распределение значений атрибута. И в том, и в другом случае максимальное количество поддерживаемых состояний зависит от значения параметра алгоритма *MAXIMUM_STATES* . Если количество состояний конкретного атрибута превышает значение параметра алгоритма *MAXIMUM_STATES* , то выбираются наиболее популярные или релевантные состояния такого атрибута в количестве, равном максимально возможному количеству состояний. Для целей анализа остальные состояния группируются как пропущенные значения.  
  
 Алгоритм затем использует значение параметра *HIDDEN_NODE_RATIO* при определении начального количества нейронов, необходимых для создания скрытого слоя. Для интерпретации нейронной сети как логистической регрессии можно задать параметр *HIDDEN_NODE_RATIO* равным 0, чтобы предотвратить в сетях создание скрытого слоя, создаваемого алгоритмом для модели интеллектуального анализа данных.  
  
 Поставщик алгоритмов циклично оценивает вес всех одновременно появляющихся в сети входов. Для этого берется набор предварительно зарезервированных обучающих данных и сравнивается фактическое известное значение каждого объекта в составе контрольных данных с прогнозом сети, — этот процесс известен как *пакетное обучение*. После оценки всего набора обучающих данных алгоритм просматривает прогнозированное и фактическое значение каждого нейрона. Алгоритм вычисляет погрешность, если таковая имеется, и регулирует весовые значения, связанные с входами для данного нейрона, обрабатывая сначала выходные, а затем входные нейроны, — этот процесс известен как *обратное распространение*. Алгоритм затем повторяет процесс в отношении всего набора обучающих данных. Поскольку алгоритм может поддерживать множество весовых значений и выходных нейронов, то для управления процессом обучения, присваивающим и оценивающим весовые значения входов, используется алгоритм сопряженных градиентов. В задачи настоящей документации не входит рассмотрение алгоритма сопряженных градиентов.  
  
### <a name="feature-selection"></a>Выбор компонентов  
 Если количество входных или прогнозируемых атрибутов превышает соответственно значение параметра *MAXIMUM_INPUT_ATTRIBUTES* или *MAXIMUM_OUTPUT_ATTRIBUTES* , то для снижения сложности сетей, включенных в модель интеллектуального анализа данных, используется алгоритм выбора компонентов. Выбор компонентов уменьшает количество входных или прогнозируемых атрибутов, ограничивая их теми атрибутами, которые с точки зрения статистики наиболее релевантны для такой модели.  
  
 Выбор компонентов автоматически применяется всеми алгоритмами интеллектуального анализа данных служб [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] для улучшения качества анализа и снижения вычислительной нагрузки. Применяемый метод выбора компонентов в моделях нейронных сетей зависит от типа данных атрибута. Для справки, следующая таблица показывает методы выбора компонентов, применяемые в моделях нейронных сетей, а также методы выбора компонентов, используемые для алгоритма логистической регрессии, основанного на алгоритме нейронной сети.  
  
|Алгоритм|Метод анализа|Комментарии|  
|---------------|------------------------|--------------|  
|Нейронная сеть|Оценка интересности<br /><br /> Энтропия Шеннона<br /><br /> Алгоритм Байеса с априорной оценкой K2<br /><br /> Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)|В алгоритме нейронных сетей могут применяться оба метода: на основе энтропии и байесовских оценок — при условии, что данные содержат непрерывные столбцы.<br /><br /> По умолчанию.|  
|Логистическая регрессия|Оценка интересности<br /><br /> Энтропия Шеннона<br /><br /> Алгоритм Байеса с априорной оценкой K2<br /><br /> Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)|Возможность передать параметр в этот алгоритм для управления поведением при выборе характеристик отсутствует, поэтому используются значения по умолчанию. Таким образом, если все атрибуты являются дискретными или дискретизированными, то по умолчанию используется метод BDEU.|  
  
 Выбором компонентов в модели нейронной сети управляют следующие параметры алгоритма: MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES и MAXIMUM_STATES. Число скрытых слоев можно также регулировать с помощью параметра HIDDEN_NODE_RATIO.  
  
### <a name="scoring-methods"></a>Методы количественной оценки  
 *Количественная оценка* представляет собой разновидность нормализации, и в контексте модели нейронной сети означает процесс преобразования величины (например дискретной текстовой метки) в значение, которое можно сравнить с другими типами входов и присвоить ему определенный вес в сети. Например, если один входной атрибут — «Пол», с возможными значениями «Мужской» и «Женский», а другой атрибут — «Доход» с переменным диапазоном значений, величины этих двух атрибутов нельзя напрямую сравнивать между собой; их нужно закодировать на некоторой общей шкале, чтобы иметь возможность сравнить их веса. Количественной оценкой называется процесс нормализации таких входов для превращения их в числовые значения, а именно в вероятностный диапазон. Функции, которые используются для нормализации, помогают также более равномерно распределять входные значения на однородной шкале, чтобы экстремальные значения не искажали результатов анализа.  
  
 Выходы нейронной сети также закодированы. Если у выхода только одна цель (то есть прогноз) или несколько целей, которые используются только для прогноза, но не для входа, модель создает одну сеть, в которой нормализация, возможно, не потребуется. Однако если несколько атрибутов используются для ввода и прогнозирования, модель создает несколько сетей; тогда все величины должны быть нормализованы и выходы закодированы в момент выхода из сети.  
  
 Кодирование входов производится суммированием всех дискретных величин в обучающих вариантах и умножением этой величины на ее вес. Это называется *взвешенной суммой*. Она передается функции активации в скрытом слое. Для кодирования используется z-показатель, как показано ниже.  
  
 **Дискретные значения**  
  
 `μ = p` — первоначальная вероятность состояния  
  
 `StdDev  = sqrt(p(1-p))`  
  
 **Непрерывные величины**  
  
 Значение существует= `1 - μ/σ`  
  
 Нет существующего значения= `-μ/σ`  
  
 После кодирования всех величин производится взвешенное суммирование входов, в котором в качестве весов используются интенсивности ребер сети.  
  
 Кодирование выходов производится с помощью сигмоидальной функции, которую очень удобно использовать для прогнозов. Одно из полезных свойств этой функции заключается в том, что, независимо от масштабирования исходных данных и от того, положительны или отрицательны значения, результат этой функции всегда является числовым значением в диапазоне от 0 до 1. Это полезно при оценке вероятностей. Другое полезное свойство сигмоидальной функции — ее сглаживающий эффект: по мере того, как величины удаляются от точки перегиба, вероятность данного значения движется от 0 к 1, но плавно.  
  
## <a name="customizing-the-neural-network-algorithm"></a>Настройка алгоритма нейронной сети (Майкрософт)  
 Алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ) поддерживает несколько параметров, которые влияют на поведение, производительность и точность итоговой модели интеллектуального анализа данных. Можно также изменять способ обработки данных в модели, устанавливая на столбцах флаги моделирования или устанавливая флаги распределения, чтобы задать способы обработки значений столбцов.  
  
### <a name="setting-algorithm-parameters"></a>Задание параметров алгоритма  
 В следующей таблице описываются параметры, которые можно использовать с алгоритмом нейронной сети (Майкрософт).  
  
 HIDDEN_NODE_RATIO  
 Указывает соотношение скрытых нейронов к входным и выходным нейронам. Следующая формула определяет начальное количество нейронов в скрытом слое:  
  
 HIDDEN_NODE_RATIO * SQRT (количество входных нейронов \* количество выходных нейронов)  
  
 Значение по умолчанию — 4,0.  
  
 HOLDOUT_PERCENTAGE  
 Указывает процент вариантов в составе обучающих данных, используемых для вычисления ошибки контрольных данных, которая применяется как один из критериев остановки во время обучения модели интеллектуального анализа данных.  
  
 Значение по умолчанию — 30.  
  
 HOLDOUT_SEED  
 Указывает значение, используемое генератором псевдослучайных чисел в качестве начального, когда алгоритм случайным образом задает контрольные данные. При установке данного параметра равным 0 алгоритм формирует начальное значение на основе имени модели интеллектуального анализа данных, что гарантирует неизменность содержимого модели при повторной обработке.  
  
 Значение по умолчанию — 0.  
  
 MAXIMUM_INPUT_ATTRIBUTES  
 Определяет максимальное количество входных атрибутов, которое может быть задано для алгоритма до использования выбора компонентов. Установка этого значения равным 0 отключает выбор компонентов для входных атрибутов.  
  
 Значение по умолчанию — 255.  
  
 MAXIMUM_OUTPUT_ATTRIBUTES  
 Определяет максимальное количество выходных атрибутов, которое может быть задано для алгоритма до использования выбора компонентов. Установка этого значения равным 0 отключает выбор компонентов для выходных атрибутов.  
  
 Значение по умолчанию — 255.  
  
 MAXIMUM_STATES  
 Указывает максимальное число дискретных состояний на один атрибут, поддерживаемое алгоритмом. Если число состояний конкретного атрибута превышает число, указанное для данного параметра, то алгоритм использует наиболее популярные состояния такого атрибута и считает остальные состояния пропущенными.  
  
 Значение по умолчанию ― 100.  
  
 SAMPLE_SIZE  
 Указывает количество вариантов, которые будут использоваться для обучения модели. Алгоритм использует меньшее из двух значений — либо число, либо заданный параметром HOLDOUT_PERCENTAGE процент от общего количества вариантов, не включенных в состав контрольных данных.  
  
 Другими словами, если для параметра HOLDOUT_PERCENTAGE задано значение 30, то алгоритм будет использовать либо значение этого параметра, либо значение, равное 70 процентам от общего количества вариантов, в зависимости от того, какое из двух указанных значений меньше.  
  
 Значение по умолчанию — 10 000.  
  
### <a name="modeling-flags"></a>Флаги моделирования  
 Далее перечислены флаги модели, которые поддерживает алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ).  
  
 NOT NULL  
 Указывает, что столбец не может принимать значение NULL. Если во время обучения модели службы Analysis Services обнаружат значение NULL, возникнет ошибка.  
  
 Применяется к столбцам структуры интеллектуального анализа данных.  
  
 MODEL_EXISTENCE_ONLY  
 Указывает, что модель должна принимать во внимание только существование или отсутствие значения для данного атрибута. Конкретное значение не играет роли.  
  
 Применяется к столбцам модели интеллектуального анализа данных.  
  
### <a name="distribution-flags"></a>Флаги распределения  
 Далее перечислены флаги распределения, которые поддерживает алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ). Модель рассматривает эти флаги только как указания; если алгоритм обнаруживает иное распределение, то использует его, не учитывая указания.  
  
 Нормальный  
 Указывает, что величины в столбце должны обрабатываться так, как если бы они представляли собой нормальное, или гауссово, распределение.  
  
 Равномерное  
 Указывает, что значения этого столбца следует обрабатывать, как если бы они были распределены равномерно; то есть, как если бы вероятность появления любого значения была бы примерно одинаковой и являлась функцией от общего числа значений.  
  
 Логарифмическое нормальное  
 Указывает, что значения этого столбца следует обрабатывать, как если бы они были распределены по *логарифмически нормальной* кривой; то есть логарифм этих значений имеет нормальное распределение.  
  
## <a name="requirements"></a>Требования  
 Модель нейронной сети должна содержать по крайней мере один входной столбец и один выходной столбец.  
  
### <a name="input-and-predictable-columns"></a>Входные и прогнозируемые столбцы  
 Алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ) поддерживает определенные входные столбцы данных и прогнозируемые столбцы, которые перечислены ниже в таблице.  
  
|Столбец|Типы содержимого|  
|------------|-------------------|  
|Входной атрибут|Continuous, Cyclical, Discrete, Discretized, Key, Table и Ordered|  
|Прогнозируемый атрибут|Continuous, Cyclical, Discrete, Discretized и Ordered|  
  
> [!NOTE]  
>  Типы содержимого Cyclical и Ordered поддерживаются, но алгоритм обрабатывает их как дискретные величины и не производит их особой обработки.  
  
## <a name="see-also"></a>См. также  
 [Алгоритм нейронной сети Майкрософт](../../analysis-services/data-mining/microsoft-neural-network-algorithm.md)   
 [Содержимое модели интеллектуального анализа данных для модели нейронной сети & #40; Службы Analysis Services — Интеллектуальный анализ данных & #41;](../../analysis-services/data-mining/mining-model-content-for-neural-network-models-analysis-services-data-mining.md)   
 [Примеры запросов к модели нейронной сети](../../analysis-services/data-mining/neural-network-model-query-examples.md)  
  
  
